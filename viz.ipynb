{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from model import ResNet50Encoder\n",
    "from dataset import build_dataset\n",
    "from nce import nce_retrieval\n",
    "from caption_encoder import CaptionEncoder\n",
    "import mixed_precision\n",
    "\n",
    "SEQ_LEN = 20\n",
    "N_RKHS = 1024\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if device == 'cuda':\n",
    "    mixed_precision.enable_mixed_precision()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_encoder = CaptionEncoder(N_RKHS, SEQ_LEN, device=device, hidden_size=4096)\n",
    "resnet50 = ResNet50Encoder(encoder_size=128, n_rkhs=N_RKHS, ndf=128 )\n",
    "resnet50.to(device)\n",
    "resnet50, _ = mixed_precision.initialize(resnet50, None)\n",
    "\n",
    "ckpt = torch.load('checkpoints/ck4lxlpp_model.pth')\n",
    "resnet50.load_state_dict(ckpt['resnet50'])\n",
    "caption_encoder.fc.load_state_dict(ckpt['caption_fc'])\n",
    "print('Checkpoint loaded.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "batch_size = 1000\n",
    "INTERP = 3\n",
    "\n",
    "class VizTransforms:\n",
    "    '''\n",
    "    ImageNet dataset, for use with 128x128 full image encoder.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        post_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self.test_transform = transforms.Compose([\n",
    "            transforms.Resize(146, interpolation=INTERP),\n",
    "            transforms.CenterCrop(128),\n",
    "            post_transform\n",
    "        ])\n",
    "\n",
    "        self.raw_trans = transforms.Compose([\n",
    "            transforms.Resize(256, interpolation=INTERP),\n",
    "            transforms.CenterCrop(256),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        \n",
    "    \n",
    "    def __call__(self, inp):\n",
    "        out = self.test_transform(inp)\n",
    "        raw = self.raw_trans(inp)\n",
    "        return out, raw\n",
    "\n",
    "transforms128 = VizTransforms()\n",
    "test_dataset = datasets.CocoCaptions(\n",
    "                    root=os.path.expanduser('~/data/coco/val2017'), \n",
    "                    annFile=os.path.expanduser('~/data/coco/annotations/captions_val2017.json'), \n",
    "                    transform=transforms128)\n",
    "\n",
    "loader = \\\n",
    "    torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=True,\n",
    "                                pin_memory=True,\n",
    "                                drop_last=True,\n",
    "                                num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(transformed_imgs, raw_imgs), captions = next(iter(loader))\n",
    "transformed_imgs = transformed_imgs.to(device)\n",
    "encoded_imgs, r7 = resnet50(transformed_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline\n",
    "\n",
    "def nce_retrieval_reverse(encoded_images, encoded_queries, top_k=5):\n",
    "    batch_size = encoded_images.size(0)\n",
    "    n_rkhs = encoded_images.size(1)\n",
    "    n_queries = encoded_queries.size(0)\n",
    "\n",
    "    # (bs, 1, 1, rkhs) -> (bs, rkhs)\n",
    "    encoded_images = encoded_images.reshape(batch_size, n_rkhs)\n",
    "    encoded_images = F.normalize(encoded_images)\n",
    "\n",
    "    scores = torch.mm(encoded_images, encoded_queries.t())\n",
    "    cos_sims_idx = torch.sort(scores, dim=1, descending=True)[1]\n",
    "    cos_sis_idx = cos_sims_idx[:, :top_k]\n",
    "    return cos_sis_idx\n",
    "\n",
    "def show(img):\n",
    "    npimg = img.numpy()\n",
    "    fig, ax = plt.subplots(figsize=(30, 60))\n",
    "    ax.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
    "    \n",
    "def visualize(encoded_queries, encoded_imgs, raw_imgs):    \n",
    "    top_k = 5\n",
    "    top_k_idx = nce_retrieval(encoded_imgs, encoded_queries, top_k)\n",
    "    top_k_idx = torch.flatten(top_k_idx)\n",
    "    matches = raw_imgs[top_k_idx]\n",
    "    viz = make_grid(matches, nrow=top_k)\n",
    "    show(viz.cpu()) \n",
    "\n",
    "def visualize_captions(raw_captions, encoded_queries, encoded_imgs, raw_imgs):\n",
    "    viz = make_grid(raw_imgs, nrow=raw_imgs.size(0))\n",
    "    show(viz.cpu()) \n",
    "    top_k = 5\n",
    "    top_k_idx = nce_retrieval_reverse(encoded_imgs, encoded_queries, top_k)\n",
    "    for i, e in enumerate(top_k_idx):\n",
    "        print('\\n'.join([raw_captions[idx] for idx in e]))\n",
    "        print('-----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caption -> Image retrival\n",
    "queries = [\n",
    "   'tennis'\n",
    "]\n",
    "encoded_queries, _ = caption_encoder(queries)\n",
    "visualize(encoded_queries, encoded_imgs, raw_imgs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image -> Caption retrieval\n",
    "encoded_queries, _ = caption_encoder(captions[0])\n",
    "visualize_captions(captions[0], encoded_queries, encoded_imgs[0:6], raw_imgs[0:6])    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
